{"title":"Robots.txt","tag":"glossary/robots.txt","post":"\n\n{{Sidebar(\"Glossary\")}}\n\nA **robots.txt** is a file which is usually placed in the root of a website (for example, `https://www.example.com/robots.txt`).\nIt specifies whether {{Glossary(\"crawler\", \"crawlers\")}} are allowed or disallowed from accessing an entire website or to certain resources on a website.\nA restrictive `robots.txt` file can prevent bandwidth consumption by crawlers.\n\nA site owner can forbid crawlers to detect a certain path (and all files in that path) or a specific file.\nThis is often done to prevent these resources from being indexed or served by search engines.\n\nIf a crawler is allowed to access resources, you can define [indexing rules](/blog/Web/HTTP/Reference/Headers/X-Robots-Tag#directives) for those resources via `<meta name=\"robots\">` elements and {{HTTPHeader(\"X-Robots-Tag\")}} HTTP headers.\nSearch-related crawlers use these rules to determine how to index and serve resources in search results, or to adjust the crawl rate for specific resources over time.\n\n## See also\n\n- {{HTTPHeader(\"X-Robots-Tag\")}}\n- {{Glossary(\"Search engine\")}}\n- {{RFC(\"9309\", \"Robots Exclusion Protocol\")}}\n- [How Google interprets the robots.txt specification](https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt) on developers.google.com\n- https://www.robotstxt.org\n- [Robots.txt](https://en.wikipedia.org/wiki/Robots.txt) on Wikipedia\n","slug":"bafc09a564d245f9b9557666330d1d90","authors":"Admin Es Solution","draft":false,"datecreated":"2025-07-05T10:01:11.987Z","dateupdated":"2025-07-05T10:01:11.988Z","layout":"PostBanner"}